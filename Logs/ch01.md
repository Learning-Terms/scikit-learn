# 决策树

- /sklearn/tree/decision_tree.py

## 基本概念

- 参考：[决策树（理论）](https://blog.csdn.net/the_ZED/article/details/129290733?ops_request_misc=%257B%2522request%255Fid%2522%253A%252246b48cd93362a65efaa3979d99beab59%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=46b48cd93362a65efaa3979d99beab59&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-129290733-null-null.142^v102^pc_search_result_base4&utm_term=%E5%86%B3%E7%AD%96%E6%A0%91&spm=1018.2226.3001.4187)

### 何为决策树
决策树（Decision Tree）是一种**分类和回归**方法，是基于各种情况发生的所需条件构成决策树，以实现**期望最大化**的一种图解法。
由于这种决策分支画成图形很像一棵树的枝干，故称决策树。它的运行机制非常通俗易懂，因此被誉为机器学习中，最“友好”的算法。
下面通过一个简单的例子来阐述它的执行流程。
假设根据大量数据（含 3 个指标：天气、温度、风速）构建了一棵“可预测学校会不会举办运动会”的决策树（如下图所示）。

![1](https://i-blog.csdnimg.cn/blog_migrate/53b100ca513694280eb2acb552ad9cac.png#pic_center)

接下来，当我们拿到某个数据时，就能做出对应预测。
在对任意数据进行预测时，都需要**从决策树的根结点开始**，**一步步走到叶子结点**（执行决策的过程）。
如，对下表中的第一条数据（ [ 阴天，寒冷，强 ] ）：首先从根结点出发，判断 “天气” 取值，而该数据的 “天气” 属性取值为 “阴天”，从决策树可知，
此时可直接输出决策结果为 “举行”。这时，无论其他属性取值为什么，都不需要再执行任何决策（类似于 “短路” 现象）。

![2](https://i-blog.csdnimg.cn/blog_migrate/993a17760c57248767116e1cc521d934.png#pic_center)

### 决策树的组成
决策树由**结点**和**有向边**组成。结点有两种类型：**内部结点（圆）和叶结点（矩形）**。
其中，内部结点表示一个**特征（属性）**；叶结点表示一个**类别**。而有向边则对应其所属内部结点的**属性的取值范围**。

![3](https://i-blog.csdnimg.cn/blog_migrate/64b0af3ecbae3d9807119c6aa0805964.png#pic_center)

- 在用决策树进行分类时，首先从根结点出发，对实例在该结点的对应属性进行测试，接着会根据测试结果，将实例分配到其子结点；
- 然后，在子结点继续执行这一流程，如此**递归地**对实例进行测试并分配，直至到达**叶结点**；最终，该实例将被分类到叶结点所指示的结果中。

- 在决策树中，若把每个内部结点视为一个条件，每对结点之间的有向边视为一个选项，则从根结点到叶结点的每一条路径都可以看做是一个规则，
- 而叶结点则对应着在指定规则下的结论。这样的规则具有互斥性和完备性，从根结点到叶结点的每一条路径代表了一类实例，并且这个实例只能在这条路径上。
- 从这个角度来看，决策树相当于是一个 if-then 的规则集合，因此它具有非常好的可解释性（白盒模型），这也是为什么说它是机器学习算法中最“友好”的一个原因。

### 决策树的构建
决策树的本质是**从训练集中归纳出一套分类规则**，使其尽量符合以下要求：

- 具有较好的泛化能力；
- 在 1 的基础上尽量不出现过拟合现象。

注意到一件事：当目标数据的**特征较多**时，构建的具有不同规则的决策树也相当庞大（**成长复杂度为 𝑂(𝑛!)** ）。如当仅考虑 5 个特征时，就能构建出 5×4×3×2×1=120 种。在这么多树中，选择哪一棵才能达到最好的分类效果呢？实际上，这个问题的本质是：**应该将样本数据的特征按照怎样的顺序添加到一颗决策树的各级结点中？** 这便是构建决策树所需要关注的问题核心。

- 如，在前面的例子中，为什么要先对“天气”进行划分，然后再是“温度”和“风速”呢（下图1）？可不可以先对“风速”进行划分，然后再是“温度”和“天气”呢（下图）？

![4](https://i-blog.csdnimg.cn/blog_migrate/e9f94738463ff7f9a779a6befbec0092.png#pic_center)

- 一种很直观的思路是：如果按照某个特征对数据进行划分时，它能最大程度地将原本混乱的结果尽可能划分为几个**有序**的大类，则就应该先以这个特征为决策树中的**根结点**。接着，**不断重复这一过程**，直到整棵决策树被构建完成为止。

- 基于此，引入信息论中的“熵”。

## 熵

### 熵的作用

**熵（Entropy)是表示随机变量不确定性的度量**。说简单点就是**物体内部的混乱程度**。比如下边的两幅图中，从 图1 到 图2 表示了熵增的过程。对于决策树的某个结点而言，它在对样本数据进行分类后，我们当然希望分类后的结果能使得整个样本集在各自的类别中尽可能有序，**即希望某个特征在被用于分类后，能最大程度地降低样本数据的熵,因为我们最终的目的就是获得一个确定的结果**。

![5](https://i-blog.csdnimg.cn/blog_migrate/3c1fb61fb983c8db1f83f1e09e2dfc0e.png#pic_center)

现在假设有这样一个待分类数据（如下图所示），若分类器 1 选择特征 𝑥1、分类器 2 选择特征 𝑥2 分别为根构建了一棵决策树，其效果如下：

![6](https://i-blog.csdnimg.cn/blog_migrate/2686120ddaba3cb64bc6ebf72363bd26.png#pic_center)

则根据以上结果，可以很直观地认为，决策树 2 的分类效果优于决策树 1 。从熵的角度看，决策树 2 在通过特征 𝑥2 进行分类后，**整个样本被划分为两个分别有序的类簇**；而决策树 1 在通过特征 𝑥1 进行分类后，**得到的分类结果依然混乱（甚至有熵增的情况），因此这个特征在现阶段被认为是无效特征**。

### 熵的定义

故此，可以这样认为：构建决策树的实质是**对特征进行层次选择**，而衡量特征选择的合理性指标，则是**熵**。为便于说明，下面先给出熵的定义：设 𝑋 是取值在有限范围内的一个**离散随机变量**，其概率密度为：

$$
P(X=x_i)=p_i, i=1,2,...,n
$$

下图展示了有关 $p_i$ 与k的定义（其中，𝑘是该集合中元素的类别数）：

![7](https://i-blog.csdnimg.cn/blog_migrate/fb48a36493a00504e635a9388f4eb0e8.png#pic_center)

则随机变量 𝑋 的熵定义为:

$$
H(X)=-\sum_{i=1}^{k} p_i logp_i
$$

![8](https://i-blog.csdnimg.cn/blog_migrate/eed3d5049876e71890706e6858013a20.png#pic_center)

此公式可以良好的反映“熵值越大，事物越混乱”这一理论。

### 熵的计算
例如，现在有两个集合：𝐴 = { 1,2 } ， 𝐵 = { 1,2,3,4,5,6 } ，若以这两个集合为取值空间，则可分别计算其熵。
- 对于集合 𝐴 ，先计算其分布列为：

![9](https://i-blog.csdnimg.cn/blog_migrate/f85d7d4c48cb75edb8793cfc2265e4bc.png#pic_center)

于是可得到其熵为： 



