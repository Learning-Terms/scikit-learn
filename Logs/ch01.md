# 决策树

- /sklearn/tree/decision_tree.py

## 基本概念

- 参考：[决策树（理论）](https://blog.csdn.net/the_ZED/article/details/129290733?ops_request_misc=%257B%2522request%255Fid%2522%253A%252246b48cd93362a65efaa3979d99beab59%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=46b48cd93362a65efaa3979d99beab59&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-129290733-null-null.142^v102^pc_search_result_base4&utm_term=%E5%86%B3%E7%AD%96%E6%A0%91&spm=1018.2226.3001.4187)

### 何为决策树
决策树（Decision Tree）是一种**分类和回归**方法，是基于各种情况发生的所需条件构成决策树，以实现**期望最大化**的一种图解法。
由于这种决策分支画成图形很像一棵树的枝干，故称决策树。它的运行机制非常通俗易懂，因此被誉为机器学习中，最“友好”的算法。
下面通过一个简单的例子来阐述它的执行流程。
假设根据大量数据（含 3 个指标：天气、温度、风速）构建了一棵“可预测学校会不会举办运动会”的决策树（如下图所示）。

![1](https://i-blog.csdnimg.cn/blog_migrate/53b100ca513694280eb2acb552ad9cac.png#pic_center)

接下来，当我们拿到某个数据时，就能做出对应预测。
在对任意数据进行预测时，都需要**从决策树的根结点开始**，**一步步走到叶子结点**（执行决策的过程）。
如，对下表中的第一条数据（ [ 阴天，寒冷，强 ] ）：首先从根结点出发，判断 “天气” 取值，而该数据的 “天气” 属性取值为 “阴天”，从决策树可知，
此时可直接输出决策结果为 “举行”。这时，无论其他属性取值为什么，都不需要再执行任何决策（类似于 “短路” 现象）。

![2](https://i-blog.csdnimg.cn/blog_migrate/993a17760c57248767116e1cc521d934.png#pic_center)

### 决策树的组成
决策树由**结点**和**有向边**组成。结点有两种类型：**内部结点（圆）和叶结点（矩形）**。
其中，内部结点表示一个**特征（属性）**；叶结点表示一个类别。而有向边则对应其所属内部结点的可选项（属性的取值范围）。

![3](https://i-blog.csdnimg.cn/blog_migrate/64b0af3ecbae3d9807119c6aa0805964.png#pic_center)

在用决策树进行分类时，首先从根结点出发，对实例在该结点的对应属性进行测试，接着会根据测试结果，将实例分配到其子结点；
然后，在子结点继续执行这一流程，如此递归地对实例进行测试并分配，直至到达叶结点；最终，该实例将被分类到叶结点所指示的结果中。

在决策树中，若把每个内部结点视为一个条件，每对结点之间的有向边视为一个选项，则从根结点到叶结点的每一条路径都可以看做是一个规则，
而叶结点则对应着在指定规则下的结论。这样的规则具有互斥性和完备性，从根结点到叶结点的每一条路径代表了一类实例，并且这个实例只能在这条路径上。
从这个角度来看，决策树相当于是一个 if-then 的规则集合，因此它具有非常好的可解释性（白盒模型），这也是为什么说它是机器学习算法中最“友好”的一个原因。
